{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@markdown #**Exploring DQD Algorithms** 🔬"
      ],
      "metadata": {
        "id": "VIv9Brvlk-b3",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr-94yAuzYns",
        "outputId": "45e233e2-1a9f-411c-e20e-9634a05aea37",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-5a113c18-ce54-64b9-f13d-14d3e5c8b89d)\n"
          ]
        }
      ],
      "source": [
        "#@markdown #**Check GPU type** 🕵️\n",
        "#@markdown ### Factory reset runtime if you don't have the desired GPU.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown V100 = Excellent (*Available only for Colab Pro users*)\n",
        "\n",
        "#@markdown P100 = Very Good\n",
        "\n",
        "#@markdown T4 = Good (*preferred*)\n",
        "\n",
        "#@markdown K80 = Meh\n",
        "\n",
        "#@markdown P4 = (*Not Recommended*) \n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJZEFUAfXX2z",
        "outputId": "bb7ed1c5-40a9-4350-9bee-dd65a98d6c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.9.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
            "\u001b[K     |█████████████                   | 834.1 MB 1.4 MB/s eta 0:14:42tcmalloc: large alloc 1147494400 bytes == 0x3a7f4000 @  0x7fcde9f5d615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |████████████████▌               | 1055.7 MB 1.3 MB/s eta 0:12:47tcmalloc: large alloc 1434370048 bytes == 0x7ee4a000 @  0x7fcde9f5d615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |█████████████████████           | 1336.2 MB 1.3 MB/s eta 0:09:06tcmalloc: large alloc 1792966656 bytes == 0x3c7c000 @  0x7fcde9f5d615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |██████████████████████████▌     | 1691.1 MB 1.2 MB/s eta 0:04:46tcmalloc: large alloc 2241208320 bytes == 0x6ea64000 @  0x7fcde9f5d615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |████████████████████████████████| 2041.3 MB 1.3 MB/s eta 0:00:01tcmalloc: large alloc 2041315328 bytes == 0xf43c6000 @  0x7fcde9f5c1e7 0x4b2590 0x4b261c 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6\n",
            "tcmalloc: large alloc 2551644160 bytes == 0x1e233c000 @  0x7fcde9f5d615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x4bad99 0x4d3249\n",
            "\u001b[K     |████████████████████████████████| 2041.3 MB 8.7 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.10.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (20.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.6 MB 71.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1+cu111) (4.1.1)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.1+cu111) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.1+cu111) (1.21.6)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.9.1+cu111 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.1+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.1+cu111 torchvision-0.10.1+cu111\n",
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 1.12 MiB | 30.21 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 236, done.\u001b[K\n",
            "remote: Total 236 (delta 0), reused 0 (delta 0), pack-reused 236\u001b[K\n",
            "Receiving objects: 100% (236/236), 8.92 MiB | 36.38 MiB/s, done.\n",
            "Resolving deltas: 100% (122/122), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/CLIP\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.9.1+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.10.1+cu111)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Installing collected packages: ftfy, clip\n",
            "  Running setup.py develop for clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.5.0-py3-none-any.whl (36 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 39.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: ninja, einops\n",
            "Successfully installed einops-0.5.0 ninja-1.11.1\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    93    0    93    0     0     67      0 --:--:--  0:00:01 --:--:--    67\n",
            "100  363M  100  363M    0     0  28.9M      0  0:00:12  0:00:12 --:--:-- 36.1M\n",
            "Cloning into 'pyribs'...\n",
            "remote: Enumerating objects: 2852, done.\u001b[K\n",
            "remote: Counting objects: 100% (2852/2852), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1056/1056), done.\u001b[K\n",
            "remote: Total 2852 (delta 1865), reused 2648 (delta 1736), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2852/2852), 31.94 MiB | 39.55 MiB/s, done.\n",
            "Resolving deltas: 100% (1865/1865), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./pyribs\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "\u001b[33mWARNING: ribs 0.4.0 does not provide the extra 'all'\u001b[0m\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (1.21.6)\n",
            "Collecting numpy_groupies>=0.9.16\n",
            "  Downloading numpy_groupies-0.9.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (0.56.4)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (1.3.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (2.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (3.1.0)\n",
            "Collecting semantic-version>=2.10\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs==0.4.0) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs==0.4.0) (4.13.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs==0.4.0) (0.39.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->ribs==0.4.0) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->ribs==0.4.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.0->ribs==0.4.0) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ribs==0.4.0) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.51.0->ribs==0.4.0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.51.0->ribs==0.4.0) (3.10.0)\n",
            "Building wheels for collected packages: ribs\n",
            "  Building wheel for ribs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ribs: filename=ribs-0.4.0-py3-none-any.whl size=76025 sha256=5aa4f120a40066566dbc322b5698b62cf2b67d77ce8cf3d431ad6db41a62c86a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bq_1gx4k/wheels/c9/11/4b/66717ff9f4bdcff8cba45cb0ca191db0b4e56d17b9e9461229\n",
            "Successfully built ribs\n",
            "Installing collected packages: semantic-version, numpy-groupies, ribs\n",
            "Successfully installed numpy-groupies-0.9.20 ribs-0.4.0 semantic-version-2.10.0\n"
          ]
        }
      ],
      "source": [
        "#@markdown #**Install libraries** 🏗️\n",
        "# @markdown This cell will take around 4 minutes to download several libraries.\n",
        "\n",
        "#@markdown ---\n",
        "!pip install --upgrade torch==1.9.1+cu111 torchvision==0.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip install -e ./CLIP\n",
        "!pip install einops ninja\n",
        "!curl -LO 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-1024x1024.pkl'\n",
        "\n",
        "!git clone https://github.com/icaros-usc/pyribs.git\n",
        "!pip install ./pyribs[all]\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"./CLIP\")\n",
        "sys.path.append(\"./stylegan2-ada-pytorch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vYIf5bwsYPI3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import clip\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from einops import rearrange\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm_notebook, tqdm\n",
        "from PIL import Image\n",
        "\n",
        "from ribs.archives import CVTArchive, GridArchive\n",
        "from ribs.emitters import GradientAborescenceEmitter\n",
        "from ribs.schedulers import Scheduler\n",
        "from ribs.visualize import grid_archive_heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N0lkdRW5YFBk"
      },
      "outputs": [],
      "source": [
        "def tensor_to_pil_img(img):\n",
        "    img = (img.clamp(-1, 1) + 1) / 2.0\n",
        "    img = img[0].permute(1, 2, 0).detach().cpu().numpy() * 255\n",
        "    img = Image.fromarray(img.astype('uint8'))\n",
        "    return img\n",
        "\n",
        "def norm1(prompt):\n",
        "    return prompt / prompt.square().sum(dim=-1, keepdim=True).sqrt()\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "def cos_sim_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().mul(2)\n",
        "\n",
        "def prompts_dist_loss(x, targets, loss):\n",
        "    distances = [loss(x, target) for target in targets]\n",
        "    return torch.stack(distances, dim=-1).sum(dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FEptLHtqYE2Z"
      },
      "outputs": [],
      "source": [
        "class MakeCutouts(torch.nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.0):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, x):\n",
        "        sideY, sideX = x.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([]) ** self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = x[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "make_cutouts = MakeCutouts(224, 32, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "29vaSPaNYEt2"
      },
      "outputs": [],
      "source": [
        "class CLIP(object):\n",
        "    def __init__(self, device='cpu'):\n",
        "        self.device = device\n",
        "        clip_model_name = \"ViT-B/32\"\n",
        "        self.model, _ = clip.load(clip_model_name, device=device)\n",
        "        self.model = self.model.requires_grad_(False)\n",
        "        self.model.eval()\n",
        "        self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                              std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def embed_text(self, prompt):\n",
        "        return norm1(self.model.encode_text(clip.tokenize(prompt)\n",
        "               .to(self.device)).float())\n",
        "\n",
        "    def embed_cutout(self, image):\n",
        "        return norm1(self.model.encode_image(self.normalize(image)))\n",
        "\n",
        "    def embed_image(self, image):\n",
        "        n = image.shape[0]\n",
        "        cutouts = make_cutouts(image)\n",
        "        embeds = self.embed_cutout(cutouts)\n",
        "        embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
        "        return embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "96S2T01tYElA"
      },
      "outputs": [],
      "source": [
        "class Generator(object):\n",
        "\n",
        "    def __init__(self, device='cpu'):\n",
        "        self.device = device\n",
        "        model_filename = 'stylegan2-ffhq-1024x1024.pkl'\n",
        "        with open(model_filename, 'rb') as fp:\n",
        "            self.model = pickle.load(fp)['G_ema'].to(device)\n",
        "            self.model.eval()\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad_(False)\n",
        "        self.init_stats()\n",
        "        self.latent_shape = (-1, 512)\n",
        "\n",
        "    def init_stats(self):\n",
        "        zs = torch.randn([10000, self.model.mapping.z_dim], device=self.device)\n",
        "        ws = self.model.mapping(zs, None)\n",
        "        self.w_stds = ws.std(0)\n",
        "        qs = ((ws - self.model.mapping.w_avg) / self.w_stds).reshape(10000, -1)\n",
        "        self.q_norm = torch.norm(qs, dim=1).mean() * 0.15\n",
        "\n",
        "    def gen_random_ws(self, num_latents):\n",
        "        zs = torch.randn([num_latents, self.model.mapping.z_dim], device=self.device)\n",
        "        ws = self.model.mapping(zs, None)\n",
        "        return ws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zGvAiVz7YEWK"
      },
      "outputs": [],
      "source": [
        "class Classifier(object):\n",
        "\n",
        "    def __init__(self, gen_model, class_model, celebrity_id='Musk'):\n",
        "        self.device = gen_model.device\n",
        "        self.gen_model = gen_model\n",
        "        self.class_model = class_model\n",
        "        self.measures = []\n",
        "        if celebrity_id == 'Beyonce':\n",
        "            self.init_objective('A photo of the face of Beyonce.')\n",
        "            self.add_measure('A photo of Beyonce as a small child.', \n",
        "                             'A photo of Beyonce as an elderly person.')\n",
        "            self.add_measure('A photo of Beyonce with short hair.', \n",
        "                             'A photo of Beyonce with long hair.')\n",
        "        elif celebrity_id == 'Cruise':\n",
        "            self.init_objective('A photo of the face of Tom Cruise.')\n",
        "            self.add_measure('A photo of Tom Cruise as a small child.', \n",
        "                             'A photo of Tom Cruise as an elderly person.')\n",
        "            self.add_measure('A photo of Tom Cruise with short hair.', \n",
        "                             'A photo of Tom Cruise with long hair.')\n",
        "        elif celebrity_id == 'Lopez':\n",
        "            self.init_objective('A photo of the face of Jennifer Lopez.')\n",
        "            self.add_measure('A photo of Jennifer Lopez as a small child.', \n",
        "                             'A photo of Jennifer Lopez as an elderly person.')\n",
        "            self.add_measure('A photo of Jennifer Lopez with short hair.', \n",
        "                             'A photo of Jennifer Lopez with long hair.')\n",
        "        elif celebrity_id == 'Musk':\n",
        "            self.init_objective('A photo of the face of Elon Musk.')\n",
        "            self.add_measure('A photo of Elon Musk as a small child.', \n",
        "                             'A photo of Elon Musk as an elderly person.')\n",
        "            self.add_measure('A photo of Elon Musk with short hair.', \n",
        "                             'A photo of Elon Musk with long hair.')\n",
        "        else:\n",
        "            print('The celebrity \\\"{}\\\" is not a valid option.'.format(celebrity_id))\n",
        "            exit(0)\n",
        "\n",
        "    def init_objective(self, text_prompt):\n",
        "        texts = [frase.strip() for frase in text_prompt.split(\"|\") if frase]\n",
        "        self.obj_targets = [self.class_model.embed_text(text) for text in texts]\n",
        "\n",
        "    def add_measure(self, positive_text, negative_text):\n",
        "        texts = [frase.strip() for frase in positive_text.split(\"|\") if frase]\n",
        "        negative_targets = [self.class_model.embed_text(text) for text in texts]\n",
        "        \n",
        "        texts = [frase.strip() for frase in negative_text.split(\"|\") if frase]\n",
        "        positive_targets = [self.class_model.embed_text(text) for text in texts]\n",
        "        \n",
        "        self.measures.append((negative_targets, positive_targets))\n",
        "\n",
        "    def find_good_start_latent(self, batch_size=16, num_batches=32):\n",
        "        with torch.inference_mode():\n",
        "            qs = []\n",
        "            losses = []\n",
        "            G = self.gen_model.model\n",
        "            w_stds = self.gen_model.w_stds\n",
        "            for _ in range(num_batches):\n",
        "                q = (G.mapping(torch.randn([batch_size, G.mapping.z_dim], device=self.device),\n",
        "                    None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds\n",
        "                images = G.synthesis(q * w_stds + G.mapping.w_avg)\n",
        "                embeds = self.class_model.embed_image(images.add(1).div(2))\n",
        "                loss = prompts_dist_loss(embeds, self.obj_targets, spherical_dist_loss).mean(0)\n",
        "                i = torch.argmin(loss)\n",
        "                qs.append(q[i])\n",
        "                losses.append(loss[i])\n",
        "            qs = torch.stack(qs)\n",
        "            losses = torch.stack(losses)\n",
        "\n",
        "            i = torch.argmin(losses)\n",
        "            q = qs[i].unsqueeze(0)\n",
        "\n",
        "        return q.flatten()\n",
        "\n",
        "    def generate_image(self, latent_code):\n",
        "        ws, _ = self.transform_to_w([latent_code])\n",
        "        images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
        "        return images\n",
        "\n",
        "    def transform_to_w(self, latent_codes):\n",
        "        qs = []\n",
        "        ws = []\n",
        "        for cur_code in latent_codes:\n",
        "            q = torch.tensor(\n",
        "                    cur_code.reshape(self.gen_model.latent_shape), \n",
        "                    device=self.device,\n",
        "                    requires_grad=True,\n",
        "                )\n",
        "            qs.append(q)\n",
        "            w = q * self.gen_model.w_stds + self.gen_model.model.mapping.w_avg\n",
        "            ws.append(w)\n",
        "\n",
        "        ws = torch.stack(ws, dim=0)\n",
        "        return ws, qs\n",
        "\n",
        "    def compute_objective_loss(self, embeds, qs, dim=None):\n",
        "        loss = prompts_dist_loss(embeds, self.obj_targets, spherical_dist_loss).mean(0)\n",
        "\n",
        "        diff = torch.max(torch.norm(qs, dim=dim), self.gen_model.q_norm)\n",
        "        reg_loss = (diff - self.gen_model.q_norm).pow(2)\n",
        "        loss = loss + 0.1 * reg_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def compute_objective(self, sols):\n",
        "        ws, qs = self.transform_to_w(sols)\n",
        "\n",
        "        images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
        "        embeds = self.class_model.embed_image(images.add(1).div(2))\n",
        "    \n",
        "        loss = self.compute_objective_loss(embeds, qs[0])\n",
        "        loss.backward()\n",
        "\n",
        "        value = loss.cpu().detach().numpy()\n",
        "        jacobian = -qs[0].grad.cpu().detach().numpy()\n",
        "        return value, jacobian.flatten()\n",
        "\n",
        "    def compute_measure(self, index, sols):\n",
        "        ws, qs = self.transform_to_w(sols)\n",
        "\n",
        "        images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
        "        embeds = self.class_model.embed_image(images.add(1).div(2))\n",
        "\n",
        "        measure_targets = self.measures[index]\n",
        "        pos_loss = prompts_dist_loss(embeds, measure_targets[0], cos_sim_loss).mean(0)\n",
        "        neg_loss = prompts_dist_loss(embeds, measure_targets[1], cos_sim_loss).mean(0)\n",
        "        loss = pos_loss - neg_loss\n",
        "        loss.backward()\n",
        "\n",
        "        value = loss.cpu().detach().numpy()\n",
        "        jacobian = qs[0].grad.cpu().detach().numpy()\n",
        "        return value, jacobian.flatten()\n",
        "\n",
        "    def compute_measures(self, sols):\n",
        "    \n",
        "        values = []\n",
        "        jacobian = []\n",
        "        for i in range(len(self.measures)):\n",
        "            value, jac = self.compute_measure(i, sols)\n",
        "            values.append(value)\n",
        "            jacobian.append(jac)\n",
        "\n",
        "        return np.stack(values, axis=0), np.stack(jacobian, axis=0)\n",
        "\n",
        "    def compute_all(self, sols):\n",
        "        with torch.inference_mode():\n",
        "\n",
        "            ws, qs = self.transform_to_w(sols)\n",
        "            qs = torch.stack(qs, dim=0)\n",
        "\n",
        "            images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
        "            embeds = self.class_model.embed_image(images.add(1).div(2))\n",
        "            \n",
        "            values = []\n",
        "            loss = self.compute_objective_loss(embeds, qs, dim=(1,2))\n",
        "            value = loss.cpu().detach().numpy()\n",
        "            values.append(value)\n",
        "\n",
        "            for i in range(len(self.measures)):\n",
        "                measure_targets = self.measures[i]\n",
        "                pos_loss = prompts_dist_loss(\n",
        "                        embeds, \n",
        "                        measure_targets[0], \n",
        "                        cos_sim_loss,\n",
        "                    ).mean(0)\n",
        "                neg_loss = prompts_dist_loss(\n",
        "                        embeds, \n",
        "                        measure_targets[1], \n",
        "                        cos_sim_loss\n",
        "                    ).mean(0)\n",
        "                loss = pos_loss - neg_loss\n",
        "                value = loss.cpu().detach().numpy()\n",
        "                values.append(value)\n",
        "\n",
        "        return np.stack(values, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d-RMRh5kXS8h"
      },
      "outputs": [],
      "source": [
        "def transform_obj(objs):\n",
        "    # Remap the objective from minimizing [0, 20] to maximizing [0, 100]\n",
        "    return (20.0-objs)*5.0\n",
        "\n",
        "def create_optimizer(algorithm, classifier, seed):\n",
        "    \"\"\"Creates an optimizer based on the algorithm name.\n",
        "\n",
        "    Args:\n",
        "        algorithm (str): Name of the algorithm passed into sphere_main.\n",
        "        classifier (Classifier): The models for the search.\n",
        "        seed (int): Main seed or the various components.\n",
        "    Returns:\n",
        "        Scheduler: A ribs Scheduler for running the algorithm.\n",
        "    \"\"\"\n",
        "    bounds = [(-0.3, 0.3), (-0.3, 0.3)]\n",
        "    initial_sol = classifier.find_good_start_latent().cpu().detach().numpy()\n",
        "    dim = len(initial_sol)\n",
        "    batch_size = 36\n",
        "    num_emitters = 1\n",
        "    resolution = 200\n",
        "    grid_dims = (resolution, resolution)\n",
        "\n",
        "    # Create archive.\n",
        "    archive = GridArchive(\n",
        "        solution_dim = dim,\n",
        "        dims=grid_dims, \n",
        "        ranges=bounds, \n",
        "        learning_rate=0.02,\n",
        "        threshold_min=0.0,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # Maintain a passive elitist archive\n",
        "    passive_archive = GridArchive(\n",
        "        solution_dim = dim,\n",
        "        dims=grid_dims,\n",
        "        ranges=bounds,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # Create emitters. Each emitter needs a different seed, so that they do not\n",
        "    # all do the same thing.\n",
        "    emitter_seeds = [None] * num_emitters if seed is None else list(\n",
        "        range(seed, seed + num_emitters))\n",
        "    emitters = [\n",
        "        GradientAborescenceEmitter(\n",
        "            archive=archive,\n",
        "            x0=initial_sol,\n",
        "            sigma0=10.0,\n",
        "            step_size=0.03,\n",
        "            restart_rule='basic',\n",
        "            batch_size=batch_size,\n",
        "            seed=s\n",
        "        ) for s in emitter_seeds\n",
        "    ]\n",
        "\n",
        "    return Scheduler(archive, emitters), passive_archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1w0sfOW5X8_B"
      },
      "outputs": [],
      "source": [
        "def save_heatmap(archive, heatmap_path):\n",
        "    \"\"\"Saves a heatmap of the archive to the given path.\n",
        "    Args:\n",
        "        archive (GridArchive or CVTArchive): The archive to save.\n",
        "        heatmap_path: Image path for the heatmap.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    grid_archive_heatmap(archive, vmin=0, vmax=100, cmap=\"viridis\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(heatmap_path)\n",
        "    plt.close(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sphere(solution_batch):\n",
        "    \"\"\"Sphere function evaluation and measures for a batch of solutions.\n",
        "\n",
        "    Args:\n",
        "        solution_batch (np.ndarray): (batch_size, dim) batch of solutions.\n",
        "    Returns:\n",
        "        objective_batch (np.ndarray): (batch_size,) batch of objectives.\n",
        "        measures_batch (np.ndarray): (batch_size, 2) batch of measures.\n",
        "    \"\"\"\n",
        "    dim = solution_batch.shape[1]\n",
        "\n",
        "    # Shift the Sphere function so that the optimal value is at x_i = 2.048.\n",
        "    sphere_shift = 5.12 * 0.4\n",
        "\n",
        "    # Normalize the objective to the range [0, 100] where 100 is optimal.\n",
        "    best_obj = 0.0\n",
        "    worst_obj = (-5.12 - sphere_shift)**2 * dim\n",
        "    raw_obj = np.sum(np.square(solution_batch - sphere_shift), axis=1)\n",
        "    objective_batch = (raw_obj - worst_obj) / (best_obj - worst_obj) * 100\n",
        "\n",
        "    # Compute gradient of the objective\n",
        "    objective_grad_batch = -2 * (solution_batch - sphere_shift)\n",
        "\n",
        "    # Calculate measures.\n",
        "    clipped = solution_batch.copy()\n",
        "    clip_indices = np.where(np.logical_or(clipped > 5.12, clipped < -5.12))\n",
        "    clipped[clip_indices] = 5.12 / clipped[clip_indices]\n",
        "    measures_batch = np.concatenate(\n",
        "        (\n",
        "            np.sum(clipped[:, :dim // 2], axis=1, keepdims=True),\n",
        "            np.sum(clipped[:, dim // 2:], axis=1, keepdims=True),\n",
        "        ),\n",
        "        axis=1,\n",
        "    )\n",
        "\n",
        "    # Compute gradient of the measures\n",
        "    derivatives = np.ones(solution_batch.shape)\n",
        "    derivatives[clip_indices] = -5.12 / np.square(solution_batch[clip_indices])\n",
        "\n",
        "    mask_0 = np.concatenate((np.ones(dim // 2), np.zeros(dim - dim // 2)))\n",
        "    mask_1 = np.concatenate((np.zeros(dim // 2), np.ones(dim - dim // 2)))\n",
        "\n",
        "    d_measure0 = derivatives * mask_0\n",
        "    d_measure1 = derivatives * mask_1\n",
        "\n",
        "    measures_grad_batch = np.stack((d_measure0, d_measure1), axis=1)\n",
        "\n",
        "    return (\n",
        "        objective_batch,\n",
        "        objective_grad_batch,\n",
        "        measures_batch,\n",
        "        measures_grad_batch,\n",
        "    )"
      ],
      "metadata": {
        "id": "3_OjN-vvJR-_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Cg6QcVj0v2-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b579051c-718e-463f-b5e4-81a9143623c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:07<00:00, 47.6MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n"
          ]
        }
      ],
      "source": [
        "# Initialization.\n",
        "celebrity='Musk'\n",
        "outdir='logs'\n",
        "            \n",
        "# Create a shared logging directory for the experiments for this algorithm.\n",
        "s_logdir = os.path.join(outdir, \"cma_mae\")\n",
        "logdir = Path(s_logdir)\n",
        "outdir = Path(outdir)\n",
        "if not outdir.is_dir():\n",
        "    outdir.mkdir()\n",
        "if not logdir.is_dir():\n",
        "    logdir.mkdir()\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "clip_model = CLIP(device=device)\n",
        "gen_model = Generator(device=device)\n",
        "classifier = Classifier(gen_model, clip_model, celebrity_id=celebrity)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#StyleGAN+CLIP LSI experiments.\n",
        "trials=2\n",
        "init_pop=100\n",
        "itrs=1000\n",
        "log_freq=1\n",
        "log_arch_freq=1000\n",
        "image_monitor=False\n",
        "image_monitor_freq=5\n",
        "seed=None\n",
        "\n",
        "for trial_id in range(trials):\n",
        "    print(\"Running trial\", trial_id)\n",
        "    # Create a directory for this specific trial.\n",
        "    s_logdir = os.path.join(outdir, \"cma_mae\", f\"trial_{trial_id}\")\n",
        "    logdir = Path(s_logdir)\n",
        "    if not logdir.is_dir():\n",
        "        logdir.mkdir()\n",
        "\n",
        "    # Create a directory for logging intermediate images if the monitor is on.\n",
        "    if image_monitor:\n",
        "        image_monitor_freq = max(1, image_monitor_freq)\n",
        "        gen_output_dir = os.path.join('generations')\n",
        "        logdir = Path(gen_output_dir)\n",
        "        if not logdir.is_dir():\n",
        "            logdir.mkdir()\n",
        "        gen_output_dir = os.path.join('generations', f\"trial_{trial_id}\")\n",
        "        logdir = Path(gen_output_dir)\n",
        "        if not logdir.is_dir():\n",
        "            logdir.mkdir()\n",
        "\n",
        "    # Create a new summary file\n",
        "    summary_filename = os.path.join(s_logdir, \"summary.csv\")\n",
        "    if os.path.exists(summary_filename):\n",
        "        os.remove(summary_filename)\n",
        "    with open(summary_filename, 'w') as summary_file:\n",
        "        writer = csv.writer(summary_file)\n",
        "        writer.writerow(['Iteration', 'QD-Score', 'Coverage', 'Maximum', 'Average'])\n",
        "    \n",
        "    scheduler, passive_archive = create_optimizer(algorithm=\"cma_mae\", classifier=classifier,seed = seed)\n",
        "    archive = scheduler.archive\n",
        "\n",
        "    best = -1000\n",
        "    non_logging_time = 0.0\n",
        "    for itr in tqdm(range(1, itrs + 1)):\n",
        "        itr_start = time.time()\n",
        "        \n",
        "        solution_batch = scheduler.ask_dqd()\n",
        "        (objective_batch, objective_grad_batch, measures_batch,\n",
        "         measures_grad_batch) = sphere(solution_batch)\n",
        "        objective_grad_batch = np.expand_dims(objective_grad_batch, axis=1)\n",
        "        jacobian_batch = np.concatenate(\n",
        "                (objective_grad_batch, measures_grad_batch), axis=1)\n",
        "        scheduler.tell_dqd(objective_batch, measures_batch, jacobian_batch)\n",
        "\n",
        "        sols = scheduler.ask()\n",
        "        values = classifier.compute_all(sols)\n",
        "        values = np.transpose(values)\n",
        "\n",
        "        objs = values[:,0]\n",
        "        measures = values[:,1:3]\n",
        "\n",
        "        objs = transform_obj(np.array(objs, dtype=np.float32))\n",
        "        measures = np.array(measures, dtype=np.float32)\n",
        "        best_gen = max(objs) \n",
        "        best = max(best, best_gen)\n",
        "\n",
        "        scheduler.tell(objs, measures)\n",
        "\n",
        "        non_logging_time += time.time() - itr_start\n",
        "\n",
        "        if itr%50 == 0:\n",
        "          print('best', best, best_gen)\n",
        "\n",
        "        if image_monitor and itr % image_monitor_freq == 0:\n",
        "            best_index = np.argmax(objs)\n",
        "            latent_code = sols[best_index]\n",
        "\n",
        "            img = classifier.generate_image(latent_code)\n",
        "            img = tensor_to_pil_img(img)\n",
        "            img.save(os.path.join(gen_output_dir, f'{itr}.png'))\n",
        "\n",
        "        # Save the archive at the given frequency.\n",
        "        # Always save on the final iteration.\n",
        "        final_itr = itr == itrs\n",
        "        if (itr > 0 and itr % log_arch_freq == 0) or final_itr:\n",
        "            # Save a full archive for analysis.\n",
        "            df = passive_archive.as_pandas(include_solutions = final_itr)\n",
        "            df.to_pickle(os.path.join(s_logdir, f\"archive_{itr:08d}.pkl\"))\n",
        "\n",
        "            # Save a heatmap image to observe how the trial is doing.\n",
        "            save_heatmap(passive_archive, os.path.join(s_logdir, f\"heatmap_{itr:08d}.png\"))\n",
        "\n",
        "        # Update the summary statistics for the archive\n",
        "        if (itr > 0 and itr % log_freq == 0) or final_itr:\n",
        "            with open(summary_filename, 'a') as summary_file:\n",
        "                writer = csv.writer(summary_file)\n",
        "\n",
        "                sum_obj = 0\n",
        "                num_filled = 0\n",
        "                num_bins = passive_archive._cells\n",
        "                for obj in passive_archive._objective_arr:\n",
        "                    num_filled += 1\n",
        "                    sum_obj += obj\n",
        "                qd_score = sum_obj / num_bins\n",
        "                average = sum_obj / num_filled\n",
        "                coverage = 100.0 * num_filled / num_bins\n",
        "                data = [itr, qd_score, coverage, best, average]\n",
        "                writer.writerow(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989,
          "referenced_widgets": [
            "efa476d97c974cc99f8a32b92098573e",
            "f7614937acf941f78a7d8d8ae1d11264",
            "b2ea7cf85ac840d9b2f9e22a8e167d3c",
            "2968458be9a74f0584cfa590abd5f1b4",
            "1af5362cec6d42009420989f95976a90",
            "b67370805cfc4d5b91c7247ccda35d28",
            "8a953193e47c4712aca1de6d4c67a5b2",
            "ed89a05da6c24f80a692ce10a42fb8d3",
            "30251e3511ed4252b8825d62cbe65100",
            "afa569e175f94df9bdb5fa346ae34bc1",
            "b5125c26663c4aaca0930607a30aac29",
            "78b65d112d334c9f88a5d51f639b2d4a",
            "bdfd313141dc44b5b7e21fa782b3882c",
            "bf4cddc137c44044b30886908affd758",
            "0b15ec632f73469ab10a610197c6a8d7",
            "8f46736121414b1f82eb338a694e8e3c",
            "a2f779ba7cd9447187c1439f286eeabe",
            "04ac9a170dba4eb09c98fa470fa2baab",
            "2b975698190a4448939fd3715203902b",
            "d7ea2973fad44ffb8dd2d85dc20119c3",
            "0b4488a245f2445ba9c37bcc3528cbed",
            "6bd9292a2c0f4fe08d6aeadef5d2b2ac"
          ]
        },
        "id": "-8dVRi1TvM_w",
        "outputId": "ac0c315b-c266-4a60-d672-2394f26121b1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running trial 0\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efa476d97c974cc99f8a32b92098573e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best 95.500275 95.49053\n",
            "best 95.501175 95.491745\n",
            "best 95.51198 95.50697\n",
            "best 95.545845 94.85664\n",
            "best 95.56583 95.43601\n",
            "best 95.58491 95.483025\n",
            "best 95.58491 95.468605\n",
            "best 95.64187 40.236244\n",
            "best 95.64187 95.457985\n",
            "best 95.64187 95.63048\n",
            "best 95.64358 95.5954\n",
            "best 95.64358 95.50144\n",
            "best 95.64358 95.492905\n",
            "best 95.64358 95.467865\n",
            "best 95.64358 95.46205\n",
            "best 95.64358 95.42095\n",
            "best 95.64358 95.4295\n",
            "best 95.64358 95.44711\n",
            "best 95.64358 95.4306\n",
            "best 95.64358 95.487976\n",
            "Running trial 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78b65d112d334c9f88a5d51f639b2d4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best 95.475075 95.4129\n",
            "best 95.52494 95.50602\n",
            "best 95.52494 95.46629\n",
            "best 95.52494 90.41577\n",
            "best 95.52494 95.44537\n",
            "best 95.52494 95.46062\n",
            "best 95.52494 91.473\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1b3312a64f4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0msols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-c61caf7f81b7>\u001b[0m in \u001b[0;36mcompute_all\u001b[0;34m(self, sols)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'const'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-54c8178a3a3a>\u001b[0m in \u001b[0;36membed_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcutouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_cutouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_cutout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcutouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'(cc n) c -> cc n c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-54c8178a3a3a>\u001b[0m in \u001b[0;36membed_cutout\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0membed_cutout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0membed_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "efa476d97c974cc99f8a32b92098573e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7614937acf941f78a7d8d8ae1d11264",
              "IPY_MODEL_b2ea7cf85ac840d9b2f9e22a8e167d3c",
              "IPY_MODEL_2968458be9a74f0584cfa590abd5f1b4"
            ],
            "layout": "IPY_MODEL_1af5362cec6d42009420989f95976a90"
          }
        },
        "f7614937acf941f78a7d8d8ae1d11264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b67370805cfc4d5b91c7247ccda35d28",
            "placeholder": "​",
            "style": "IPY_MODEL_8a953193e47c4712aca1de6d4c67a5b2",
            "value": "100%"
          }
        },
        "b2ea7cf85ac840d9b2f9e22a8e167d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed89a05da6c24f80a692ce10a42fb8d3",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30251e3511ed4252b8825d62cbe65100",
            "value": 1000
          }
        },
        "2968458be9a74f0584cfa590abd5f1b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afa569e175f94df9bdb5fa346ae34bc1",
            "placeholder": "​",
            "style": "IPY_MODEL_b5125c26663c4aaca0930607a30aac29",
            "value": " 1000/1000 [1:29:02&lt;00:00,  5.60s/it]"
          }
        },
        "1af5362cec6d42009420989f95976a90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b67370805cfc4d5b91c7247ccda35d28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a953193e47c4712aca1de6d4c67a5b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed89a05da6c24f80a692ce10a42fb8d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30251e3511ed4252b8825d62cbe65100": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afa569e175f94df9bdb5fa346ae34bc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5125c26663c4aaca0930607a30aac29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78b65d112d334c9f88a5d51f639b2d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bdfd313141dc44b5b7e21fa782b3882c",
              "IPY_MODEL_bf4cddc137c44044b30886908affd758",
              "IPY_MODEL_0b15ec632f73469ab10a610197c6a8d7"
            ],
            "layout": "IPY_MODEL_8f46736121414b1f82eb338a694e8e3c"
          }
        },
        "bdfd313141dc44b5b7e21fa782b3882c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2f779ba7cd9447187c1439f286eeabe",
            "placeholder": "​",
            "style": "IPY_MODEL_04ac9a170dba4eb09c98fa470fa2baab",
            "value": " 35%"
          }
        },
        "bf4cddc137c44044b30886908affd758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b975698190a4448939fd3715203902b",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7ea2973fad44ffb8dd2d85dc20119c3",
            "value": 354
          }
        },
        "0b15ec632f73469ab10a610197c6a8d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b4488a245f2445ba9c37bcc3528cbed",
            "placeholder": "​",
            "style": "IPY_MODEL_6bd9292a2c0f4fe08d6aeadef5d2b2ac",
            "value": " 354/1000 [31:34&lt;57:35,  5.35s/it]"
          }
        },
        "8f46736121414b1f82eb338a694e8e3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2f779ba7cd9447187c1439f286eeabe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04ac9a170dba4eb09c98fa470fa2baab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b975698190a4448939fd3715203902b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7ea2973fad44ffb8dd2d85dc20119c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b4488a245f2445ba9c37bcc3528cbed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bd9292a2c0f4fe08d6aeadef5d2b2ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}